# Welcome to MultiSOCIAL toolbox! An Open-Source Library for Quantifying Multimodal Social Interaction** 

Veronica Romero, Department of Psychology and Davis Institute for Artificial Intelligence,  Colby College, USA, vcromero@colby.edu, Tahiya Chowdhury, Davis Institute for Artificial Intelligence,  Colby College, USA, tchowdhu@colby.edu, Alexandra Paxton, Department of Psychological Science & Center for the Ecological Study of Perception and Action,  University of Connecticut, USA 

In everyday life, most of our experiences of social interaction are multimodal, but in our research, most of our studies of social interaction focus on only one kind of communication behavior. However, by reducing the dimensionality of our data, we are limited in our ability to capture the dynamics of real-world social behavior. A major reason for this unimodal focus is technological: Until recently, the richness of human behavior in just minutes of face-to-face interaction could take over an hour of meticulous hand-coding, transcription, and annotation, but advances in computing power and software innovation are changing that. Here, we present a new effort to assemble open-source tools into a single platform for multimodal interaction data: the MultiSOCIAL Toolbox (or the MULTImodal timeSeries Open-SourCe Interaction Analysis Library). While these tools exist in separate packages for scientists with programming abilities, our goal is to expand access to scholars with limited (or even non-existent) programming experience and to accelerate discovery through a unified multimodal data processing pipeline. The toolbox enables any researcher who has video files of any kind of interaction to extract time-series data in three modalities: body movement (non-verbal behavior); transcripts (what was said during interaction); and acoustic prosodic characteristics (how it was said).

